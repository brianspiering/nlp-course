{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word vector implementations\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Train on your own corpus (aka, data in NLP)\n",
    "2. Use pretrained vectors\n",
    "3. Combine 1 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Train on your own corpus \n",
    "----\n",
    "\n",
    "> \"Data is the world's best regularizer\"\n",
    "\n",
    "You need __a lot__ of data.\n",
    "\n",
    "100 billion words is good start ðŸ˜‰. 100 million will work. 10 million is the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" width=\"400\"/></center>\n",
    "Keras is a high-level neural networks API, written in Python \n",
    "\n",
    "Capable of running on top of TensorFlow, CNTK, Theano (others in the future!)\n",
    "\n",
    "Created to enable fast experimentation. \n",
    "\n",
    "Like SQL for DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at some code - https://gist.github.com/codekansas/15b3c2a2e9bc7a3c345138a32e029969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Another example](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use pretrained vectors\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Download them manually ðŸ˜¡\n",
    "------\n",
    "\n",
    "[File on Google Drive](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "via https://gist.github.com/bhaettasch/d7f4e22e79df3c8b6c20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model\n",
    "\n",
    "# XXX: This will crash your computer if it 8GB of RAM\n",
    "# news_vectors = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "news_vectors.most_similar(positive=['woman', 'king'], \n",
    "                          negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://fasttext.cc/img/fasttext-logo-color-web.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[fastText pretrained vectors](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# XXX: Way bigger, thus bigger, than Google vectors\n",
    "# ! wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load Facebook's pre-trained Word2Vec model\n",
    "# wiki_vectors = gensim.models.load_word2vec_format('wiki.en/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "https://blog.manash.me/how-to-use-pre-trained-word-vectors-from-facebooks-fasttext-a71e6d55f27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fine Tuning\n",
    "------\n",
    "\n",
    "<center><img src=\"images/pretraining.png\" width=\"700\"/></center>\n",
    "\n",
    "1. Start with pretrained vectors\n",
    "2. Then do additional training to adjust the weights for your own corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "What is the difference between Fine Tuning & Transfer Learning in Deep Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fine Tuning is adjusting all pretrained model weights. This will change how the model respresents the data.\n",
    "\n",
    "Transfer Learning is just adjusting the classification layer (typically last layer), all other layers are frozen. The respresnations don't change, just mapping to specific categorizations change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pros of Fine Tuning\n",
    "-----\n",
    "\n",
    "- Less chance of out-of-sample words\n",
    "- Robust vector representations for low frequency words\n",
    "- Faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cons of Fine Tuning\n",
    "-----\n",
    "\n",
    "- Forced to use same number of dimensions\n",
    "- Have to write your own code to implement it (if you find good fine tuning code, please share it withm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "------\n",
    "\n",
    "- Most people use pretrained vectors\n",
    "- Other people train their own vectors\n",
    "- Fine tuning is the best of both worlds!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
