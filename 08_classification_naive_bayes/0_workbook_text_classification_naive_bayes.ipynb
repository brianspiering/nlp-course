{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification\n",
    "===\n",
    "\n",
    "![](images/supervised_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By The End of This Session You Will:\n",
    "---\n",
    "- Know the the basics of text classification\n",
    "- Be able to the name the advantages and disadvantages of bag-of-words model\n",
    "- Be able to write Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Text Classification Basics\n",
    "---\n",
    "\n",
    "Text classification predicts the category of a given document based on a set of pre-defined categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary:\n",
    "---\n",
    "\n",
    "1. **Some examples of text classification tasks:**\n",
    "  - An email is \"Spam\" or \"Not Spam\"\n",
    "  - The author of a particular document\n",
    "  - The gender of a particular document\n",
    "  - If a movie review is positive or negative\n",
    "  - And many more\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "2. **The most basic method for text classification is to device rules that would differentiate the different classes of documents, for example:**\n",
    "  - \"!\" in title would indicate spam\n",
    "  - Certain usage of words would indicate one author as oppose to the other\n",
    "  - More factual style of writing would indicate a male author\n",
    "  - Words such as \"suck\" and \"terrible\" would indicate a bad movie review\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "3. **However, devicing rules tailored to each clssification task is unfeasible due to the human labor required**\n",
    "\n",
    "  <br>\n",
    "  \n",
    "4. **In general, a text classification problem has 3 components:**\n",
    "   - A document\n",
    "   - A pre-defined set of classes\n",
    "   - A training set of documents and their corresponding classes\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "5. **The output of the text classification is the predicted class of the document in question**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Check Questions\n",
    "---\n",
    "\n",
    "1) Why is it unadvisable to device specific rules for each text classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "Click here for solution to 1.\n",
    "</summary>\n",
    "`\n",
    "Specific rules have to be deviced for each classification problem and that is time consuming\n",
    "`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What are the 3 components of a text classification problem? Is it a supervised or unsupervised problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "Click here for solution 2.\n",
    "</summary>\n",
    "`\n",
    "1. A document\n",
    "2. A pre-defined set of classes\n",
    "3. A training set of documents and their corresponding classes\n",
    "\n",
    "Supervised problem since labels are provided to train the model\n",
    "`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bag of Words Representation\n",
    "---\n",
    "\n",
    "The Bag of Words Representation vectorizes a document to a vector of numbers represent the count of each vocabulary in the document\n",
    "\n",
    "---\n",
    "Summary\n",
    "----\n",
    "\n",
    "1. **The Bag of Words Representation does not take into account the order of the words in the document**\n",
    "\n",
    "   - Since the number of each vocabulary is counted for each document regardless of order\n",
    "\n",
    "   <br>\n",
    "\n",
    "2. **Say we have 2 documents below:**\n",
    "\n",
    "   1. 6004 is a fun class and I love natural language processing\n",
    "   2. Katie also like natural language processing\n",
    "\n",
    "   <br>\n",
    "\n",
    "3. **The Bag of Words Representation would be as follow:**\n",
    "\n",
    "|            | 6004 | is | a | fun | class | and | I | love | natural | language | processing | Katie | also | like |\n",
    "|------------|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "| Document 1 | 1    | 1  | 1 | 1   | 1     | 1   | 1 | 1    | 1       | 1        | 1          | 0     | 0    | 0    |\n",
    "| Document 2 | 0    | 0  | 0 | 0   | 0     | 0   | 0 | 0    | 1       | 1        | 1          | 1     | 1    | 1    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "---\n",
    "\n",
    "1. Write a function that would give the Bag of Words Representation for the following documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = ['System that replaces human intuition with algorithms outperforms human teams',\n",
    "           'MIT researchers aim to take the human element out of big-data analysis', \n",
    "           'We view the Data Science Machine as a natural complement to human intelligence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(lst):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "Click here for solution to 1.\n",
    "</summary>\n",
    "`\n",
    "docs = ['System that replaces human intuition with algorithms outperforms human teams',\n",
    "        'MIT researchers aim to take the human element out of big-data analysis', \n",
    "        'We view the Data Science Machine as a natural complement to human intelligence']\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "def bag_of_words(lst):\n",
    "    docs_tokens = [doc.split(' ') for doc in lst]\n",
    "    all_tokens = [token for tokens in docs_tokens for token in tokens]\n",
    "    vocab_dict = dict(zip(all_tokens, [0] * len(all_tokens)))\n",
    "    results = []\n",
    "    for tokens in docs_tokens:\n",
    "        dict_copy = deepcopy(vocab_dict)\n",
    "        for token in tokens:\n",
    "            dict_copy[token] += 1\n",
    "        results.append(dict_copy.values())\n",
    "    return vocab_dict.keys(), np.array(results)\n",
    "\n",
    "bag_of_words(docs)\n",
    "\n",
    "(['intuition',\n",
    "  'intelligence',\n",
    "  'big-data',\n",
    "  'System',\n",
    "  'Machine',\n",
    "  'outperforms',\n",
    "  'as',\n",
    "  'human',\n",
    "  'element',\n",
    "  'MIT',\n",
    "  'out',\n",
    "  'We',\n",
    "  'to',\n",
    "  'take',\n",
    "  'Data',\n",
    "  'that',\n",
    "  'Science',\n",
    "  'complement',\n",
    "  'with',\n",
    "  'a',\n",
    "  'replaces',\n",
    "  'natural',\n",
    "  'of',\n",
    "  'analysis',\n",
    "  'teams',\n",
    "  'aim',\n",
    "  'algorithms',\n",
    "  'the',\n",
    "  'researchers',\n",
    "  'view'],\n",
    " array([[1, 0, 0, 1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
    "         0, 0, 0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 1, 1, 1, 0, 1, 1, 0],\n",
    "        [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
    "         0, 0, 0, 0, 0, 1, 0, 1]]))\n",
    "`\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Basics\n",
    "---\n",
    "\n",
    "Naive Bayes is a common algorithm that uses the bag of word representation for text classification.\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Naive Bayes predicts the class of a document based on the probability of oberserving the class given the document**\n",
    "   - The probability of observing each possible class given the document in question is computed\n",
    "   - The prediction is the class with the highest probability given the document\n",
    "   \n",
    "   <br>\n",
    "\n",
    "2. **Naive Bayes is based on the Bayes Theorem:**\n",
    "\n",
    "  $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{p(\\text{doc})}$$\n",
    "  \n",
    "  <br>\n",
    "\n",
    "  **where**\n",
    "\n",
    "  - $p(\\text{class }| \\text{ doc})$ is the probability of observing a particular class given a document (**Posterior**)\n",
    "  - $p(\\text{doc } | \\text{ class})$ is the probability of observing a document given a particular class (**Likelihood**)\n",
    "  - $p(\\text{class})$ is the probability of observing each of the classes (**Prior**)\n",
    "  - $p(\\text{doc})$ is the probability of observing a a document\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "3. **We assume the probability of observing any given document $p(doc)$ is constant**\n",
    "\n",
    "   Hence we can simplify the above expression: \n",
    "\n",
    "   $$p(\\text{class }| \\text{ doc}) \\propto p(\\text{doc } | \\text{ class}) \\times p(\\text{class})$$\n",
    "\n",
    "   Note that we are not computing a measure of probability exactly when we make this simplification.\n",
    "   \n",
    "   However, since $p(doc)$ is constant, the most probable class (posterior) can still be decided by selecting the highest value\n",
    "   \n",
    "   <br>\n",
    "\n",
    "4. **Using the bag of words model, we can express the likelihood $p(\\text{doc }| \\text{ class})$ as following:**\n",
    "\n",
    "  $$p(\\text{doc }| \\text{ class}) = p(x_1,x_2,... ,x_n \\text{ | class})$$\n",
    "  \n",
    "  **where** $x_1, x_2, ... , x_n$ are the features in the bag of word model\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "5. **We are also assuming conditional independence for the likelihood term, where**\n",
    "  \n",
    "   - The probability of $x_1$ occurring is independent of $x_2$ occurring given a particular class\n",
    "   - Hence we are able to express the likelihood as following:\n",
    "     \n",
    "     $$p(x_1,x_2,... ,x_n \\text{ | class}) = p(x_1 \\text{ | class}) \\cdot p(x_2 \\text{ | class}) \\cdot ... \\cdot p(x_n \\text{ | class})$$\n",
    "     \n",
    "     <br>\n",
    "     \n",
    "   - **This assumption is mostly incorrect**, since we can easily think of how the occurrence of some words affects that of the others in a given class\n",
    "   - But this simplification allow us to compute the posterior easily without sacrificing too much predictive power\n",
    "   \n",
    "   <br>\n",
    "     \n",
    "6. **Hence we are able to express the posterior as following:**\n",
    "\n",
    "   $$p(\\text{class }| \\text{ doc}) \\propto p(\\text{class}) \\times (p(x_1 \\text{ | class}) \\cdot p(x_2 \\text{ | class}) \\cdot ... \\cdot p(x_n \\text{ | class}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Questions\n",
    "----\n",
    "\n",
    "1) What theorem is Naive Bayes based on ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) State the term that represents likelihood and explain in plain English what likelihood is ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) State the term that represents the posterior probability and explain in plain English what posterior is ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) State the term that represents the prior probability and explain in plain English what prior is ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) State the assumptions that Naive Bayes make when used with the Bag of Words Representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "Click here for solution to 1.\n",
    "</summary>\n",
    "`\n",
    "Bayes Theorem\n",
    "`\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Click here for solution to 2.\n",
    "</summary>\n",
    "`\n",
    "Likelihood is the probability of observing the data given a class. In the context of text classification, the probability of observing the document (the bag of words) given the class.\n",
    "`\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Click here for solution to 3.\n",
    "</summary>\n",
    "`\n",
    "Posterior is the probability of observing a certain class given the data. In the context of text classification, the probability of observing the class of a document given the document.\n",
    "`\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Click here for solution to 4.\n",
    "</summary>\n",
    "`\n",
    "Prior is the probability of observing each of the classes regardless of the data. In the context of text classification, the probability of observing the class of a document. Imagine drawing a document randomly and recording the class that it belongs to\n",
    "`\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Click here for solution to 5.\n",
    "</summary>\n",
    "`\n",
    "Naive Bayes, used with bag of words, assumes the order of the words does not matter to the prediction. It also assume conditional independence between the words such that the occurrences of words do not affect each other in a given class.\n",
    "`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "Summary\n",
    "----\n",
    "\n",
    "__Text classification__:\n",
    "- Text classification is the task of predicting the category of a given document\n",
    "- It requires a document, a pre-defined set of classes and a set of documents that are labeled with the pre-defined classes\n",
    "- Text classification is a supervised machine learning problem \n",
    "\n",
    "  <br>\n",
    "__Bag of Words__:\n",
    "- Bag of Words is a basic way of featurizing / vectorizing a document into a matrix of numbers\n",
    "- The count of each vocabulary in the corpus (collection of documents) is tallied for each of the documents\n",
    "- The information about the order of the words is lost in the Bag of Words Representation\n",
    "\n",
    "  <br>\n",
    "__Naive Bayes__:\n",
    "- Naive Bayes is a common machine learning algorithm used in text classification\n",
    "- Naive Bayes based on the __Bayes Theorem__\n",
    "- __Posterior probability__ is calculated for each of the class given a document\n",
    "- __Posterior probability__ is the probability of observing a class given the document\n",
    "- Class with highest __Posterior probability__ is decided as the predicted class of the document\n",
    "- __Likelihood__ and __Prior__ is used to calculate the **Posterior**\n",
    "- __Likelihood__ is probability of obeserving the data given a class \n",
    "- __Prior__ the probability of observing a class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
