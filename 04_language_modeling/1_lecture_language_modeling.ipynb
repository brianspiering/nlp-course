{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Language Modeling (LM)\n",
    "-----\n",
    "<center><img src=\"https://www.sri.com/sites/default/files/uploads/brains_languagetrans_istockphoto.jpg\" width=\"700\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define Language Modeling\n",
    "- Calculate the maximum-likelihood estimation (MLE) for words in a given corpus\n",
    "- Define and calculate ngrams\n",
    "- Explain Markov modeling\n",
    "- Appy the Markov model to language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is Language Modeling (LM)? \n",
    "----\n",
    "\n",
    "Assign probabilities to sequences of \"words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given the \"words\" seen thus far, what \"word\" is mostly to appear next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__What is next word in this sequence?__\n",
    "\n",
    "`5, 4, 3, 2, _`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Levels of Language Modeling\n",
    "------\n",
    "\n",
    "1. characters\n",
    "2. word \n",
    "3. ngram\n",
    "4. \"refrigerator magnets\" model\n",
    "5. phrases\n",
    "6. sentences\n",
    "7. conversation (groups of sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is a ngram?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A sliding window of tokens\n",
    "\n",
    "<center><img src=\"https://www.usenix.org/legacy/event/sec08/tech/full_papers/small/small_html/variable_length_tokens.png\" style=\"width: 350px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is unigram modeling?\n",
    "----\n",
    "\n",
    "Predicts the probability of the next word based on the probability of each word appearing.\n",
    "\n",
    "No conditional probability - All independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What is another name for an unigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is a bigram model?\n",
    "-------\n",
    "\n",
    "Predicts the probability of the next word based on the previous word \n",
    "\n",
    "Conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What is a trigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predicts the probability of the next word based on the previous two words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparing Unigram, Bigram, Trigram models\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"images/compare.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are the limitations of ngram modeling?\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "N-grams models do __not__ account for syntax dependencies.\n",
    "\n",
    "It is a pure statistical language modeling approach. There is no theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do we build a Language Model (LM)?\n",
    "-----\n",
    "\n",
    "> Language is its own best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Get enough data to depict typical (and atypical) language use accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LM as Refrigerator Magnets \n",
    "-------\n",
    "<br>\n",
    "<center><img src=\"images/magents.png\" width=\"700\"/></center>\n",
    "\n",
    "A language is a collection of highly likely n-grams.\n",
    "\n",
    "Idioms are examples - \"kick the bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov Assumption\n",
    "----\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/AAMarkov.jpg/220px-AAMarkov.jpg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the Markov assumption?\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "> The future is independent of the past given the present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Markov model\n",
    "-----\n",
    "\n",
    "The conditional probability distribution of future states of a processs depends only upon the present state, not on the sequence of events that preceded it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assumption that a node only depends on its immediate parents, not on all predecessors in the ordering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is a memoryless property of a stochastic process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can __not__ escape the chain rule. ⛓\n",
    "------\n",
    "\n",
    "The probability of the next word is computed using the Chain Rule\n",
    "<center><img src=\"images/chain_rule.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1st-order Markov chain\n",
    "------\n",
    "\n",
    "Using the Markov assumption and the chain rule: \n",
    "\n",
    "<center><img src=\"images/first_order.png\" width=\"700\"/></center>\n",
    "\n",
    "The probability of choosing a word from a given vocabulary is equal to the probability of that word occurring times the product of previous sliding bigram sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "NLP is just time series in disguise\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Math Flashback (you have been warned!)\n",
    "-----\n",
    "\n",
    "(first-order) Markov chain is characterized by an initial distribution over states and a state transition matrix \n",
    "\n",
    "<center><img src=\"images/markov_matrix.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2nd order Markov chain\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/markov_2order.png\" width=\"1000\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Look back is limited by k (0 (unigram), 1 (bigram), 2 (trigram), ...)\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/word_prediction.png\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What limits our models from using a large value of `k`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We typically don't have enough data.\n",
    "\n",
    "For example, if we pick `k` to be 4. We would need counts for every possible 4 word combination in English!\n",
    "\n",
    "That is why is it easier to model characters than sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to calculate a LM\n",
    "----\n",
    "\n",
    "Language model (LM) computes `P(next word | previous words)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximum-likelihood estimation (MLE) for LM\n",
    "----\n",
    "\n",
    "<center><img src=\"images/mle_estimate.png\" width=\"700\"/></center>\n",
    "\n",
    "`<s>`: sentence start tag  | `</s>`: sentence stop tag\n",
    "\n",
    "The conditional probability is the joint probability normalized by total count of conditional item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for Understanding\n",
    "-----\n",
    "\n",
    "What is `P(<s>|</s>)`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`2/3`\n",
    "\n",
    "`</s> appears 3 times`   \n",
    "`<s> appears 2 times after </s>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for Understanding\n",
    "-----\n",
    "\n",
    "What is the `P(eggs|green)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`1/1`: Every time green appears eggs follow. (Thus all eggs are green)\n",
    "\n",
    "<center><img src=\"http://therockvillemarketfarm.com/files/2013/03/Screen-Shot-2013-03-16-at-9.18.53-AM.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Learning is Eating Machine Learning\n",
    "------\n",
    "\n",
    "<center><img src=\"http://2.bp.blogspot.com/-8GD4qeDAvow/VjRx4jVpD8I/AAAAAAAAOVM/59xpx5CH1jk/s1600/mi-01.png\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What are examples of Deep Learning Sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Log-bilinear model\n",
    "- Recurrent Neural network (RNN) \n",
    "- Long Short Term Memory (LSTM)\n",
    "- [Gated recurrent unit (GRU)](https://en.wikipedia.org/wiki/Gated_recurrent_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Always start with simpliest model that could possiblely work!\n",
    "----\n",
    "\n",
    "1. Go end-to-end fastest\n",
    "1. Understand your data through modeling\n",
    "1. Establish a baseline, then do model comparisons\n",
    "1. Fall back modeling for production ML\n",
    "    - Systems should fail gracefully.\n",
    "    - If a complex model can't render a prediction, then a simplier model might be able to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Log-bilinear model (LBL): Perhaps the simplest neural language model\n",
    "------\n",
    "\n",
    "<center><img src=\"images/lbl.png\" width=\"700\"/></center>\n",
    "\n",
    "Computes a context vector as a linear combination of the previous word vectors. \n",
    "\n",
    "And a distribution of the next word w<sub>i</sub> is computed based on similarity between the word embedding ϕ(w) and the context, by taking a softmax over the vocabulary V."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- [Log-bilinear model Source](https://www.cs.toronto.edu/~hinton/csc2535/notes/hlbl.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Check out https://github.com/minimaxir/textgenrnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Language Modeling is learning the statistically patterns in a language.\n",
    "- Then assign probabilities to sequences of symbols.\n",
    "- The probabilities can be assigned at any language level (given enough data).\n",
    "- Markov Assumption: The future only depends on the present.\n",
    "- That limitation reduces computational complex and data demands.\n",
    "- But can be extended backward to model more previous states.\n",
    "- We can use Maximum-likelihood estimation (MLE) to predict next word.\n",
    "- Use Deep Learning to get cutting edge results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Point to Ponder\n",
    "-----\n",
    "\n",
    "A language model has limited ability to generalize ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For any plausible sentence with Tuesday in it,   \n",
    "there is a similar plausible sentence with Wednesday in it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to due that, we need word tagging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generative Model vs Discriminative Model sidebar\n",
    "-----------\n",
    "\n",
    "A generative model learns the joint probability distribution p(x,y).\n",
    "\n",
    "A discriminative model learns the conditional probability distribution p(y|x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The data: (x,y): (1,0), (1,0), (2,0), (2, 1)\n",
    "\n",
    "| __p(x,y)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1/2 | 0 |\n",
    "| x=2 | 1/4 | 1/4 |\n",
    "\n",
    "\n",
    "| __p(y pipe x)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1 | 0 |\n",
    "| x=2 | 1/2 | 1/2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The distribution p(y|x) is the natural distribution for classifying a given example x into a class y, which is why algorithms that model this directly are called discriminative algorithms.\n",
    "\n",
    "Generative algorithms model p(x,y), which can be transformed into p(y|x) by applying Bayes rule and then used for classification. \n",
    "\n",
    "However, the distribution p(x,y) can also be used for other purposes. For example you could use p(x,y) to generate likely (x,y) pairs.\n",
    "\n",
    "[Source](http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
